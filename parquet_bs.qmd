---
title: "CORI/RISI Ontology"
execute:
  echo: true
  output: true
  message: false
  warning: false
format:
  html:
    code-fold: true
reference-location: margin
citation-location: margin
---


## S3 storage: 

- [ ] csv (R read into memory)  

- [ ] pmtiles ???

- [ ] does it need to download to your computer vs accessing directly to the cloud?

- [ ] parquet 

- handling permissions / credentials in R 
    * [ ] listing 
    * [ ] reading
    * [ ] writing 

- [ ] targets?

- [ ] duckDB <-> s3

- [ ] back end: arrow vs duckdb vs polars? 

# R option to interact with S3 

Intersting reading on pros and cos: https://github.com/cloudyr/aws.s3/issues/433

- [Pins](https://pins.rstudio.com/index.html): can write parquet / arrow etc, dev. by the Posit team, I have not a clear view of their dependencies ..

- [arrow's FileSystem classes](https://github.com/apache/arrow/blob/main/r/man/FileSystem.Rd) or better `help("FileSystem", package = "arrow")`

- [paws](https://www.paws-r-sdk.com/) package for the full suite of AWS services, wrapper of SDK. For s3 see [here](https://www.paws-r-sdk.com/examples/s3/).

- [aws.s3](https://github.com/cloudyr/aws.s3) seems the good call right now: 

## concepts that will need to be explain and or abstracted 

- a bucket can have multiple objects, you can get a bucket (what are you getting? raw/type of object) or a specific object

- you can also open a connection to a bucket for some stream 

a bucket can have directory and files 

## using aws.s3

Just build on top of rest API with curl

Best resource find: https://cloud.r-project.org/web/packages/aws.s3/aws.s3.pdf 

  Stuff to test:
    - [x] credentials 

    - [x] read (a bit more complicated see specify the function.) 

    - [ ] write 

    - [ ] delete 

    - [ ] permissions  

 useful functions test: 
 
```r
library("aws.s3")
bucket_exists("fcctilestest")`

get_bucket_policy("fcctilestest") # I am unclear on that  

get_bucket_policy("cori-crowe")

get_bucket(bucket = "fcctilestest") # very useful

get_bucket(bucket = "fcc-tests-s3-parquet") # very usefull 

get_acl("cori-crowe")

raw_mess <- get_object(object = "june23/state_abbr=WY/technology=10/data_0.parquet", 
          "fcc-tests-s3-parquet")

get_bucket(bucket = "proj-bead")

sf_object <- s3read_using(FUN = sf::st_read,
                          object = "processed_data/clean_ilecs.gpkg",
                          bucket = "proj-bead") 
```

## using paws

Credentials is similar than aws.s3 but build on top of SDK

https://www.paws-r-sdk.com/examples/s3/

```r
s3 <- paws::s3()

s3$list_buckets() # return a list prob because that what match a json more

bucket_name <- paste0("paws-example-", "oli-test")

s3$create_bucket(
  Bucket = bucket_name
)

```


## arrow / duckDB

- [ ] maturity on OS 

- [ ] maturity in R 

- [ ] status on spatial operations

- [ ] PG -> parquet 

- [ ] appending parquet? 



### How to partition a file:

Arrow support:

- "hive style", a key value system: `"year=2019/month=1/file.parquet"` 

- "Directory": `"2019/01/file.parquet"`

Hadley Wickham suggest:

> As a rough guide, arrow suggests that you avoid files smaller than 20MB and larger than 2GB and avoid partitions that produce more than 10,000 files. [^r4ds_partition]

[^r4ds_partition]: [https://r4ds.hadley.nz/arrow.html#partitioning](https://r4ds.hadley.nz/arrow.html#partitioning)

::: {.callout-note}
What could be a good partition scheme for FCC data?
:::



## Workflow "mind map"


```{mermaid}
flowchart LR
dplyr --> arrow
arrow --> filesystem["`**Filesystem**
    multiple csv
    parquet
    more ..`"]

arrow <-->|?| duckDB

dplyr --> dbplyr
dplyr --> DBI
dbplyr --> DBI 
DBI --> duckDB
duckDB --> filesystem["`**Filesystem**
    multiple csv
    parquet
    more ..`"]
```



## Resources

- https://www.crunchydata.com/blog/parquet-and-postgres-in-the-data-lake

- https://mastodon.cloud/@milvus/112395302626488455

- https://grantmcdermott.com/duckdb-polars/

- https://duckdb.org/docs/

- https://r4ds.hadley.nz/arrow.html

- on Arrow posts from Danielle: https://blog.djnavarro.net/posts/2022-12-26_strange-year/#writing-about-apache-arrow (usually awesome) 